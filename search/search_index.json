{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenVPN and Transmission with WebUI Docker container running Transmission torrent client with WebUI over an OpenVPN tunnel Overview \u00b6 You have found the documentation. That usually means that you either: Want to read a bit about how the image is built and how it works Want to get started, and are looking for a setup guide Already have a setup, but something is broken We'll try to address them here but no matter which one of them it is, knowing more about this image makes it easier to understand how it should be and what could be wrong. So starting with number 1 is never a bad idea. NB: These pages are under re-construction. Follow the issue here and feel free to comment or help out :) Also we just released version 3.0, so if you have some breakage - please read here . Good places to start \u00b6 The basic building blocks Running the container VPN and networking in containers Supported providers and server locations Provider specific features/instructions Configuration options list Troubleshooting \u00b6 Frequently asked questions Debugging your setup Tips & Tricks Additional features \u00b6 Web proxy: Tinyproxy RSS Plugin support","title":"Overview"},{"location":"#overview","text":"You have found the documentation. That usually means that you either: Want to read a bit about how the image is built and how it works Want to get started, and are looking for a setup guide Already have a setup, but something is broken We'll try to address them here but no matter which one of them it is, knowing more about this image makes it easier to understand how it should be and what could be wrong. So starting with number 1 is never a bad idea. NB: These pages are under re-construction. Follow the issue here and feel free to comment or help out :) Also we just released version 3.0, so if you have some breakage - please read here .","title":"Overview"},{"location":"#good_places_to_start","text":"The basic building blocks Running the container VPN and networking in containers Supported providers and server locations Provider specific features/instructions Configuration options list","title":"Good places to start"},{"location":"#troubleshooting","text":"Frequently asked questions Debugging your setup Tips & Tricks","title":"Troubleshooting"},{"location":"#additional_features","text":"Web proxy: Tinyproxy RSS Plugin support","title":"Additional features"},{"location":"building-blocks/","text":"The basic building blocks \u00b6 The goal \u00b6 The core functionality of this image is to let the user run a VPN tunnel and Transmission as easy as possible. Transmission should only run while the VPN is active and any disconnect from VPN should cause Transmission to stop. The container should provide community best practices on how to configure the kill switch, firewall and tweaks on the OpenVPN configs to make it run as fast and secure as possible. It goes like this \u00b6 To understand how it works, this is the most important events and who/what starts them. You start the container The container starts OpenVPN OpenVPN starts/stops Transmission When you start the container it is instructed to run a script to start OpenVPN. This is defined in the Dockerfile . This script is responsible for doing initial setup and prepare what is needed for OpenVPN to run successfully. Starting OpenVPN \u00b6 The main purpose of the startup script is to figure out which OpenVPN config to use. OpenVPN itself can be started with a single argument, and that is the config file. We also add a few more to tell it to start Transmission when the VPN tunnel is started and to stop Transmission when OpenVPN is stopped. That's it. Apart from that the script does some firewall config, vpn interface setup and possibly other things based on your settings. There are also some reserved script names that a user can mount/add to the container to include their own scripts as a part of the setup or teardown of the container. Anyways! You have probably seen the docker run and docker-compose configuration examples and you've put two and two together: This is where environment variables comes in. Setting environment variables is a common way to pass configuration options to containers and it is the way we have chosen to do it here. So far we've explained the need for OPENVPN_PROVIDER and OPENVPN_CONFIG . We use the combination of these two to find the right config. OPENVPN_CONFIG is not set as a mandatory option as each provider should have a default config that will be used if none is set. With the config file identified we're ready to start OpenVPN, the only thing missing are probably a username and password. There are some free providers out there, but they are the exceptions to the rule. We have to inject the username/password into the config somehow. Again there are exceptions but the majority of configs from regular providers contain a line with auth-user-pass which will make OpenVPN prompt for username and password when you start a connection. That will obviously not work for us so we need to modify that option. If it's followed by a path to a file, it will read the first line of that file as username and the second line as password. You provide your username and password as OPENVPN_USERNAME and OPENVPN_PASSWORD . These will be written into two lines in a file called /config/openvpn-credentials.txt on startup by the start script. Having written your username/password to a file, we can successfully start OpenVPN. Starting Transmission \u00b6 We're using the up option from OpenVPN to start Transmission. --up cmd Run command cmd after successful TUN/TAP device open This means that Transmission will be started when OpenVPN has connected successfully and opened the tunnel device. We are having OpenVPN call the tunnelUp.sh script which in turn will call the start scripts for Transmission and Tinyproxy . The up script will be called with a number of parameters from OpenVPN, and among them is the IP of the tunnel interface. This IP is the one we've been assigned by DHCP from the OpenVPN server we're connecting to. We use this value to override Transmissions bind address, so we'll only listen for traffic from peers on the VPN interface. The startup script checks to see if one of the alternative web ui's should be used for Transmission. It also sets up the user that Transmission should be run as, based on the PUID and PGID passed by the user along with selecting preferred logging output and a few other tweaks. Before starting Transmission we also need to see if there are any settings that should be overridden. One example of this is binding Transmission to the IP we've gotten from our VPN provider. Here we check if we find any environment variables that match a setting that we also see in settings.json. This is described in the config section . Setting a matching environment variable will then override the setting in Transmission. OpenVPN does not pass the environment variables it was started with to Transmission. To still be able to access them when starting Transmission, we're writing the ones we need to a file when starting OpenVPN. That way we can read them back and use them here. With the environment variables in place this script then overwrites the selected properties in settings.json and we're ready to start Transmission itself. After starting Transmission there is an optional step that some providers have; to get an open port and set it in Transmission. Opening a port in your local router does not work . I made that bold because it's a recurring theme. It's not intuitive until it is I guess. Since all your traffic is going through the VPN, which is kind of the point, the port you have to open is not on your router. Your router's external IP address is the destination of those packets. It is on your VPN providers end that it has to be opened. Some providers support this, other don't. We try to write scripts for those that do and that script will be executed after starting Transmission if it exists for your provider. At this point Transmission is running and everything is great! But you might not be able to access it, and that's the topic of the networking section .","title":"Image Building Blocks"},{"location":"building-blocks/#the_basic_building_blocks","text":"","title":"The basic building blocks"},{"location":"building-blocks/#the_goal","text":"The core functionality of this image is to let the user run a VPN tunnel and Transmission as easy as possible. Transmission should only run while the VPN is active and any disconnect from VPN should cause Transmission to stop. The container should provide community best practices on how to configure the kill switch, firewall and tweaks on the OpenVPN configs to make it run as fast and secure as possible.","title":"The goal"},{"location":"building-blocks/#it_goes_like_this","text":"To understand how it works, this is the most important events and who/what starts them. You start the container The container starts OpenVPN OpenVPN starts/stops Transmission When you start the container it is instructed to run a script to start OpenVPN. This is defined in the Dockerfile . This script is responsible for doing initial setup and prepare what is needed for OpenVPN to run successfully.","title":"It goes like this"},{"location":"building-blocks/#starting_openvpn","text":"The main purpose of the startup script is to figure out which OpenVPN config to use. OpenVPN itself can be started with a single argument, and that is the config file. We also add a few more to tell it to start Transmission when the VPN tunnel is started and to stop Transmission when OpenVPN is stopped. That's it. Apart from that the script does some firewall config, vpn interface setup and possibly other things based on your settings. There are also some reserved script names that a user can mount/add to the container to include their own scripts as a part of the setup or teardown of the container. Anyways! You have probably seen the docker run and docker-compose configuration examples and you've put two and two together: This is where environment variables comes in. Setting environment variables is a common way to pass configuration options to containers and it is the way we have chosen to do it here. So far we've explained the need for OPENVPN_PROVIDER and OPENVPN_CONFIG . We use the combination of these two to find the right config. OPENVPN_CONFIG is not set as a mandatory option as each provider should have a default config that will be used if none is set. With the config file identified we're ready to start OpenVPN, the only thing missing are probably a username and password. There are some free providers out there, but they are the exceptions to the rule. We have to inject the username/password into the config somehow. Again there are exceptions but the majority of configs from regular providers contain a line with auth-user-pass which will make OpenVPN prompt for username and password when you start a connection. That will obviously not work for us so we need to modify that option. If it's followed by a path to a file, it will read the first line of that file as username and the second line as password. You provide your username and password as OPENVPN_USERNAME and OPENVPN_PASSWORD . These will be written into two lines in a file called /config/openvpn-credentials.txt on startup by the start script. Having written your username/password to a file, we can successfully start OpenVPN.","title":"Starting OpenVPN"},{"location":"building-blocks/#starting_transmission","text":"We're using the up option from OpenVPN to start Transmission. --up cmd Run command cmd after successful TUN/TAP device open This means that Transmission will be started when OpenVPN has connected successfully and opened the tunnel device. We are having OpenVPN call the tunnelUp.sh script which in turn will call the start scripts for Transmission and Tinyproxy . The up script will be called with a number of parameters from OpenVPN, and among them is the IP of the tunnel interface. This IP is the one we've been assigned by DHCP from the OpenVPN server we're connecting to. We use this value to override Transmissions bind address, so we'll only listen for traffic from peers on the VPN interface. The startup script checks to see if one of the alternative web ui's should be used for Transmission. It also sets up the user that Transmission should be run as, based on the PUID and PGID passed by the user along with selecting preferred logging output and a few other tweaks. Before starting Transmission we also need to see if there are any settings that should be overridden. One example of this is binding Transmission to the IP we've gotten from our VPN provider. Here we check if we find any environment variables that match a setting that we also see in settings.json. This is described in the config section . Setting a matching environment variable will then override the setting in Transmission. OpenVPN does not pass the environment variables it was started with to Transmission. To still be able to access them when starting Transmission, we're writing the ones we need to a file when starting OpenVPN. That way we can read them back and use them here. With the environment variables in place this script then overwrites the selected properties in settings.json and we're ready to start Transmission itself. After starting Transmission there is an optional step that some providers have; to get an open port and set it in Transmission. Opening a port in your local router does not work . I made that bold because it's a recurring theme. It's not intuitive until it is I guess. Since all your traffic is going through the VPN, which is kind of the point, the port you have to open is not on your router. Your router's external IP address is the destination of those packets. It is on your VPN providers end that it has to be opened. Some providers support this, other don't. We try to write scripts for those that do and that script will be executed after starting Transmission if it exists for your provider. At this point Transmission is running and everything is great! But you might not be able to access it, and that's the topic of the networking section .","title":"Starting Transmission"},{"location":"config-options/","text":"Required environment options \u00b6 Variable Function Example OPENVPN_PROVIDER Sets the OpenVPN provider to use. OPENVPN_PROVIDER=provider . Supported providers and their config values are listed in the table above. OPENVPN_USERNAME Your OpenVPN username OPENVPN_USERNAME=asdf OPENVPN_PASSWORD Your OpenVPN password OPENVPN_PASSWORD=asdf Network configuration options \u00b6 Variable Function Example OPENVPN_CONFIG Sets the OpenVPN endpoint to connect to. OPENVPN_CONFIG=UK Southampton OPENVPN_OPTS Will be passed to OpenVPN on startup See OpenVPN doc LOCAL_NETWORK Sets the local network that should have access. Accepts comma separated list. LOCAL_NETWORK=192.168.0.0/24 CREATE_TUN_DEVICE Creates /dev/net/tun device inside the container, mitigates the need mount the device from the host CREATE_TUN_DEVICE=true Timezone option \u00b6 Set a custom timezone in tz database format. Look here for a list of valid timezones. Defaults to UTC. Variable Function Example TZ Set Timezone TZ=UTC Firewall configuration options \u00b6 When enabled, the firewall blocks everything except traffic to the peer port and traffic to the rpc port from the LOCAL_NETWORK and the internal docker gateway. If TRANSMISSION_PEER_PORT_RANDOM_ON_START is enabled then it allows traffic to the range of peer ports defined by TRANSMISSION_PEER_PORT_RANDOM_HIGH and TRANSMISSION_PEER_PORT_RANDOM_LOW. Variable Function Example ENABLE_UFW Enables the firewall ENABLE_UFW=true UFW_ALLOW_GW_NET Allows the gateway network through the firewall. Off defaults to only allowing the gateway. UFW_ALLOW_GW_NET=true UFW_EXTRA_PORTS Allows the comma separated list of ports through the firewall. Respects UFW_ALLOW_GW_NET. UFW_EXTRA_PORTS=9910,23561,443 UFW_DISABLE_IPTABLES_REJECT Prevents the use of REJECT in the iptables rules, for hosts without the ipt_REJECT module (such as the Synology NAS). UFW_DISABLE_IPTABLES_REJECT=true Health check option \u00b6 Because your VPN connection can sometimes fail, Docker will run a health check on this container every 5 minutes to see if the container is still connected to the internet. By default, this check is done by pinging google.com once. You change the host that is pinged. Variable Function Example HEALTH_CHECK_HOST this host is pinged to check if the network connection still works google.com Permission configuration options \u00b6 By default the startup script applies a default set of permissions and ownership on the transmission download, watch and incomplete directories. The GLOBAL_APPLY_PERMISSIONS directive can be used to disable this functionality. Variable Function Example GLOBAL_APPLY_PERMISSIONS Disable setting of default permissions GLOBAL_APPLY_PERMISSIONS=false Alternative web UIs \u00b6 You can override the default web UI by setting the TRANSMISSION_WEB_HOME environment variable. If set, Transmission will look there for the Web Interface files, such as the javascript, html, and graphics files. Combustion UI , Kettu , Transmission-Web-Control , and Flood come bundled with the container. You can enable one of them by setting TRANSMISSION_WEB_UI=combustion , TRANSMISSION_WEB_UI=kettu , TRANSMISSION_WEB_UI=transmission-web-control , or TRANSMISSION_WEB_UI=flood respectively. Note that this will override the TRANSMISSION_WEB_HOME variable if set. Variable Function Example TRANSMISSION_WEB_HOME Set Transmission web home TRANSMISSION_WEB_HOME=/path/to/web/ui TRANSMISSION_WEB_UI Use the specified bundled web UI TRANSMISSION_WEB_UI=combustion , TRANSMISSION_WEB_UI=kettu , TRANSMISSION_WEB_UI=transmission-web-control , or TRANSMISSION_WEB_UI=flood User configuration options \u00b6 By default everything will run as the root user. However, it is possible to change who runs the transmission process. You may set the following parameters to customize the user id that runs transmission. Variable Function Example PUID Sets the user id who will run transmission PUID=1003 PGID Sets the group id for the transmission user PGID=1003 Transmission configuration options \u00b6 In previous versions of this container the settings were not persistent but was generated from environment variables on container startup. This had the benefit of being very explicit and reproducable but you had to provide Transmission config as environment variables if you wanted them to stay that way between container restarts. This felt cumbersome to many. As of version 3.0 this is no longer true. Settings are now persisted in the /data/transmission-home folder in the container and as long as you mount /data you should be able to configure Transmission using the UI as you normally would. You may still override Transmission options by setting environment variables if that's your thing. The variables are named after the transmission config they target but are prefixed with TRANSMISSION_ , capitalized, and - is converted to _ . For example: Transmission variable name Environment variable name speed-limit-up TRANSMISSION_SPEED_LIMIT_UP speed-limit-up-enabled TRANSMISSION_SPEED_LIMIT_UP_ENABLED ratio-limit TRANSMISSION_RATIO_LIMIT ratio-limit-enabled TRANSMISSION_RATIO_LIMIT_ENABLED A full list of variables can be found in the Transmission documentation here . All variables overridden by environment variables will be logged during startup. PS: TRANSMISSION_BIND_ADDRESS_IPV4 will automatically be overridden to the IP assigned to your OpenVPN tunnel interface. This ensures that Transmission only listens for torrent traffic on the VPN interface and is part of the fail safe mechanisms. Dropping default route from iptables (advanced) \u00b6 Some VPNs do not override the default route, but rather set other routes with a lower metric. This might lead to the default route (your untunneled connection) to be used. To drop the default route set the environment variable DROP_DEFAULT_ROUTE to true . Note : This is not compatible with all VPNs. You can check your iptables routing with the ip r command in a running container. Changing logging locations \u00b6 By default Transmission will log to a file in TRANSMISSION_HOME/transmission.log . To log to stdout instead set the environment variable LOG_TO_STDOUT to true . Note : By default stdout is what container engines read logs from. Set this to true to have Tranmission logs in commands like docker logs and kubectl logs . OpenVPN currently only logs to stdout. Custom scripts \u00b6 If you ever need to run custom code before or after transmission is executed or stopped, you can use the custom scripts feature. Custom scripts are located in the /scripts directory which is empty by default. To enable this feature, you'll need to mount the /scripts directory. Once /scripts is mounted you'll need to write your custom code in the following bash shell scripts: Script Function /scripts/openvpn-pre-start.sh This shell script will be executed before openvpn start /scripts/transmission-pre-start.sh This shell script will be executed before transmission start /scripts/transmission-post-start.sh This shell script will be executed after transmission start /scripts/transmission-pre-stop.sh This shell script will be executed before transmission stop /scripts/transmission-post-stop.sh This shell script will be executed after transmission stop Don't forget to include the #!/bin/bash shebang and to make the scripts executable using chmod a+x","title":"Configuration options"},{"location":"config-options/#required_environment_options","text":"Variable Function Example OPENVPN_PROVIDER Sets the OpenVPN provider to use. OPENVPN_PROVIDER=provider . Supported providers and their config values are listed in the table above. OPENVPN_USERNAME Your OpenVPN username OPENVPN_USERNAME=asdf OPENVPN_PASSWORD Your OpenVPN password OPENVPN_PASSWORD=asdf","title":"Required environment options"},{"location":"config-options/#network_configuration_options","text":"Variable Function Example OPENVPN_CONFIG Sets the OpenVPN endpoint to connect to. OPENVPN_CONFIG=UK Southampton OPENVPN_OPTS Will be passed to OpenVPN on startup See OpenVPN doc LOCAL_NETWORK Sets the local network that should have access. Accepts comma separated list. LOCAL_NETWORK=192.168.0.0/24 CREATE_TUN_DEVICE Creates /dev/net/tun device inside the container, mitigates the need mount the device from the host CREATE_TUN_DEVICE=true","title":"Network configuration options"},{"location":"config-options/#timezone_option","text":"Set a custom timezone in tz database format. Look here for a list of valid timezones. Defaults to UTC. Variable Function Example TZ Set Timezone TZ=UTC","title":"Timezone option"},{"location":"config-options/#firewall_configuration_options","text":"When enabled, the firewall blocks everything except traffic to the peer port and traffic to the rpc port from the LOCAL_NETWORK and the internal docker gateway. If TRANSMISSION_PEER_PORT_RANDOM_ON_START is enabled then it allows traffic to the range of peer ports defined by TRANSMISSION_PEER_PORT_RANDOM_HIGH and TRANSMISSION_PEER_PORT_RANDOM_LOW. Variable Function Example ENABLE_UFW Enables the firewall ENABLE_UFW=true UFW_ALLOW_GW_NET Allows the gateway network through the firewall. Off defaults to only allowing the gateway. UFW_ALLOW_GW_NET=true UFW_EXTRA_PORTS Allows the comma separated list of ports through the firewall. Respects UFW_ALLOW_GW_NET. UFW_EXTRA_PORTS=9910,23561,443 UFW_DISABLE_IPTABLES_REJECT Prevents the use of REJECT in the iptables rules, for hosts without the ipt_REJECT module (such as the Synology NAS). UFW_DISABLE_IPTABLES_REJECT=true","title":"Firewall configuration options"},{"location":"config-options/#health_check_option","text":"Because your VPN connection can sometimes fail, Docker will run a health check on this container every 5 minutes to see if the container is still connected to the internet. By default, this check is done by pinging google.com once. You change the host that is pinged. Variable Function Example HEALTH_CHECK_HOST this host is pinged to check if the network connection still works google.com","title":"Health check option"},{"location":"config-options/#permission_configuration_options","text":"By default the startup script applies a default set of permissions and ownership on the transmission download, watch and incomplete directories. The GLOBAL_APPLY_PERMISSIONS directive can be used to disable this functionality. Variable Function Example GLOBAL_APPLY_PERMISSIONS Disable setting of default permissions GLOBAL_APPLY_PERMISSIONS=false","title":"Permission configuration options"},{"location":"config-options/#alternative_web_uis","text":"You can override the default web UI by setting the TRANSMISSION_WEB_HOME environment variable. If set, Transmission will look there for the Web Interface files, such as the javascript, html, and graphics files. Combustion UI , Kettu , Transmission-Web-Control , and Flood come bundled with the container. You can enable one of them by setting TRANSMISSION_WEB_UI=combustion , TRANSMISSION_WEB_UI=kettu , TRANSMISSION_WEB_UI=transmission-web-control , or TRANSMISSION_WEB_UI=flood respectively. Note that this will override the TRANSMISSION_WEB_HOME variable if set. Variable Function Example TRANSMISSION_WEB_HOME Set Transmission web home TRANSMISSION_WEB_HOME=/path/to/web/ui TRANSMISSION_WEB_UI Use the specified bundled web UI TRANSMISSION_WEB_UI=combustion , TRANSMISSION_WEB_UI=kettu , TRANSMISSION_WEB_UI=transmission-web-control , or TRANSMISSION_WEB_UI=flood","title":"Alternative web UIs"},{"location":"config-options/#user_configuration_options","text":"By default everything will run as the root user. However, it is possible to change who runs the transmission process. You may set the following parameters to customize the user id that runs transmission. Variable Function Example PUID Sets the user id who will run transmission PUID=1003 PGID Sets the group id for the transmission user PGID=1003","title":"User configuration options"},{"location":"config-options/#transmission_configuration_options","text":"In previous versions of this container the settings were not persistent but was generated from environment variables on container startup. This had the benefit of being very explicit and reproducable but you had to provide Transmission config as environment variables if you wanted them to stay that way between container restarts. This felt cumbersome to many. As of version 3.0 this is no longer true. Settings are now persisted in the /data/transmission-home folder in the container and as long as you mount /data you should be able to configure Transmission using the UI as you normally would. You may still override Transmission options by setting environment variables if that's your thing. The variables are named after the transmission config they target but are prefixed with TRANSMISSION_ , capitalized, and - is converted to _ . For example: Transmission variable name Environment variable name speed-limit-up TRANSMISSION_SPEED_LIMIT_UP speed-limit-up-enabled TRANSMISSION_SPEED_LIMIT_UP_ENABLED ratio-limit TRANSMISSION_RATIO_LIMIT ratio-limit-enabled TRANSMISSION_RATIO_LIMIT_ENABLED A full list of variables can be found in the Transmission documentation here . All variables overridden by environment variables will be logged during startup. PS: TRANSMISSION_BIND_ADDRESS_IPV4 will automatically be overridden to the IP assigned to your OpenVPN tunnel interface. This ensures that Transmission only listens for torrent traffic on the VPN interface and is part of the fail safe mechanisms.","title":"Transmission configuration options"},{"location":"config-options/#dropping_default_route_from_iptables_advanced","text":"Some VPNs do not override the default route, but rather set other routes with a lower metric. This might lead to the default route (your untunneled connection) to be used. To drop the default route set the environment variable DROP_DEFAULT_ROUTE to true . Note : This is not compatible with all VPNs. You can check your iptables routing with the ip r command in a running container.","title":"Dropping default route from iptables (advanced)"},{"location":"config-options/#changing_logging_locations","text":"By default Transmission will log to a file in TRANSMISSION_HOME/transmission.log . To log to stdout instead set the environment variable LOG_TO_STDOUT to true . Note : By default stdout is what container engines read logs from. Set this to true to have Tranmission logs in commands like docker logs and kubectl logs . OpenVPN currently only logs to stdout.","title":"Changing logging locations"},{"location":"config-options/#custom_scripts","text":"If you ever need to run custom code before or after transmission is executed or stopped, you can use the custom scripts feature. Custom scripts are located in the /scripts directory which is empty by default. To enable this feature, you'll need to mount the /scripts directory. Once /scripts is mounted you'll need to write your custom code in the following bash shell scripts: Script Function /scripts/openvpn-pre-start.sh This shell script will be executed before openvpn start /scripts/transmission-pre-start.sh This shell script will be executed before transmission start /scripts/transmission-post-start.sh This shell script will be executed after transmission start /scripts/transmission-pre-stop.sh This shell script will be executed before transmission stop /scripts/transmission-post-stop.sh This shell script will be executed after transmission stop Don't forget to include the #!/bin/bash shebang and to make the scripts executable using chmod a+x","title":"Custom scripts"},{"location":"debug/","text":"Debugging your setup \u00b6 The goal of this page is to provide a common set of tests that can be run to try to narrow down an issue with the container before you actually create a new issue for it. We see a lot of repeat business in the issues section and spending time answering questions for individual setups takes away from improving the container and making it more stable in the first place. This guide should be improved over time but can hopefully help you point out the most common errors and provide some pointers on how to proceed. A short summary of what you've tried should be added to the description if you can't figure out what's wrong with your setup and create an issue for it. Introduction and assumptions \u00b6 I am going to assume that you have shell access to the host and that you can use docker run commands to test it. If you're a docker-compose user then you can make a similar setup in docker-compose. If you are using any of the NAS container orchestration UIs then you just have to mimic this behaviour as best you can. Note that you can ssh into the NAS and run commands directly. NOTE: The commands listed here uses the --rm flag which will remove the container from the host when it shuts down. And as we're not mounting any volumes here, your host system will not be altered from running any of these commands. If any command breaks with this principle it will be noted. Checking that Docker works properly \u00b6 In order for this container to work you have to have a working Docker installation on your host. We'll begin very simple with this command that will print a welcome message if Docker is properly installed. docker run --rm hello-world Then we can try to run an alpine image, install curl and run curl to get your public IP. This verifies that Docker containers on your host has a working internet access and that they can look up hostnames with DNS. docker run --rm -it alpine sh -c \"apk add curl && curl ipecho.net/plain\" If you get an error with \"Could not resolve host\" then you have to look at the dns options in the Docker run reference . Finally we will check that your Docker daemon runs with a bridge network as the default network driver. The following command runs an alpine container and prints it's iptable routes. It probably outputs two lines and one of them starts with 172.x.0.0/16 dev eth0 and the other one also references 172.x.0.1 . The 172 addresses are a sign that you're on a Docker bridge network. If your local IP like 192.168.x.y shows up your container is running with host networking and the VPN container would affect the entire host instead of just affecting Transmission running within the container. docker run --rm -it alpine ip r If you have gotten any errors so far you have to refer to Docker documentation and other forums to get help getting Docker to work. Try running the container with an invalid setup \u00b6 We'll keep this brief because it's not the most useful step, but you can actually verify a bit anyways. Run this command (even if PIA is not your provider) and do not insert your real username/password: docker run --rm -it -e OPENVPN_PROVIDER=PIA -e OPENVPN_CONFIG=france -e OPENVPN_USERNAME=donald -e OPENVPN_PASSWORD=duck haugene/transmission-openvpn At this point the commands are getting longer. I'll start breaking them up into lines using \\ to escape the line breaks. For those that are new to shell commands; a \\ at the end of the line will tell the shell to keep on reading as if it was on the same line. You can copy-paste this somewhere and put everythin on the same line and remove the \\ characters if you want to. The same command then becomes: docker run --rm -it \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=donald \\ -e OPENVPN_PASSWORD=duck \\ haugene/transmission-openvpn This command should fail and exit with three lines that look like this (I've trimmed it a bit): Peer Connection Initiated with ... AUTH: Received control message: AUTH_FAILED SIGTERM[soft,auth-failure] received, process exiting And this is not nothing. The container has made contact with the VPN server and they have agreed that you do not have provided correct authentication. So we're getting somewhere. Running with a valid configuration \u00b6 Upgrading slightly from the last command we need to add our real provider, config and username/password. This is what the container needs to be able to connect to VPN. The config is not a mandatory option and all providers should have a default that is used if you don't set it. But I will set it here anyways as I think it's good to know where and what you're connecting to. The command is basically the same. I'm going to stick with PIA/france as I am a PIA user, but you should set one of the supported providers or provide your own config using the custom configuration option . Since I'm now expecting to connect successfully to my VPN provider I have to give the container elevated access to modify networking needed to establish a VPN tunnel. I'll add the --cap-add=NET_ADMIN and you can read more about that here . Also because I'm using PIA and they support port forwarding which is automatically configured in this container I will disable that script for now. It's unnecessary at this point and I don't want to introduce more error sources than I have to. docker run --rm -it --cap-add=NET_ADMIN \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=username \\ -e OPENVPN_PASSWORD=password \\ -e DISABLE_PORT_UPDATER=true \\ haugene/transmission-openvpn The logs should be longer this time, and you should end up with Initialization Sequence Completed at the end. If you don't see this message then look through your logs for errors and see if you can find your error in the list in the FAQ . If the error is not easily understandable or listed in the FAQ, open a new issue on it. Checking if Transmission is running \u00b6 We're continuing with the docker run command from the last example. Because we have not yet introduced the LOCAL_NETWORK variable you cannot access Transmission that is running in the container. We have not exposed any ports on your host either so Transmission is not reachable from outside of the container as of now. You don't need to expose Transmission outside of the container to contact it though. You can get another shell inside the same container that you are running and try to curl Transmission web ui from there. If this gets too complicated then you can skip to the next point, but please try to come back here if the next point fails. One thing is not being able to access Transmission which might be network related, but if Transmission is not running it's something else entirely. If you run docker ps you should get a list of all running docker containers. If the list is too long then you can ask for only transmission-openvpn containers with docker ps --filter ancestor=haugene/transmission-openvpn . Using the Container ID from that list you can run docker exec -it <container-id> bash . Once you're in the container run curl localhost:9091 and you should expect to get <h1>301: Moved Permanently</h1> in return. This is because Transmission runs at /web/transmission and tries to redirect you there. It doesn't matter because you now see that Transmission is running and apparently doing well. Accessing Transmission Web UI \u00b6 If you've come this far we hopefully will be able to connect to the Transmission Web UI from your browser. In order to do this we have to know what LAN IP your system is on. The reason for this is a bit complex and is described in the VPN networking section. The short version is that OpenVPN need to be able to differentiate between what traffic to tunnel and what to let go. Since the VPN is running on the Docker bridge network it is not able to detect computers on your LAN as actually being local devices. We'll base ourselves on the command from the previous sections, but to access Transmission we need to expose the 9091 port to the host and tell the containers what IP ranges NOT to tunnel. Whatever you put in LOCAL_NETWORK will be trusted as a local network and traffic to those IPs will not be tunneled. Here we will assume that you're on one of the common 192.168.x.y subnets. The command then becomes: docker run --rm -it --cap-add=NET_ADMIN \\ -p 9091:9091 \\ -e LOCAL_NETWORK=192.168.0.0/16 \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=username \\ -e OPENVPN_PASSWORD=password \\ -e DISABLE_PORT_UPDATER=true \\ haugene/transmission-openvpn With any luck you should now be able to access Transmission at http://localhost:9091 or whatever server IP where you have started the container. NOTE: If you're trying to run this beside another container you can use -p 9092:9091 to bind 9092 on the host instead of 9091 and avoid port conflict. Now what? \u00b6 If this guide has failed at some point then you should create an issue for it. Please add the command that you ran and the logs that was produced. If you're now able to access Transmission and it seems to work correctly then you should add a volume mount to the /data folder in the container. You'll then have a setup like what's shown on the main GitHub page of this project. If you have another setup that does not work then you now have two versions to compare and maybe that will lead you to find the error in your old setup. If the setup is the same but this version works then the error is in your state. Transmission stores its state in /data/transmission-home by default and it might have gotten corrupt. One simple thing to try is to delete the settings.json file that is found here. We do mess with that file and we might have corrupted it. Apart from that we do not change anything within the Transmission folder and any issues should be asked in Transmission forums. Conclusion \u00b6 I hope this has helped you to solve your problem or at least narrow down where it's coming from. If you have suggestions for improvements do not hesitate to create an issue or even better open a PR with your proposed changes.","title":"Debug your setup"},{"location":"debug/#debugging_your_setup","text":"The goal of this page is to provide a common set of tests that can be run to try to narrow down an issue with the container before you actually create a new issue for it. We see a lot of repeat business in the issues section and spending time answering questions for individual setups takes away from improving the container and making it more stable in the first place. This guide should be improved over time but can hopefully help you point out the most common errors and provide some pointers on how to proceed. A short summary of what you've tried should be added to the description if you can't figure out what's wrong with your setup and create an issue for it.","title":"Debugging your setup"},{"location":"debug/#introduction_and_assumptions","text":"I am going to assume that you have shell access to the host and that you can use docker run commands to test it. If you're a docker-compose user then you can make a similar setup in docker-compose. If you are using any of the NAS container orchestration UIs then you just have to mimic this behaviour as best you can. Note that you can ssh into the NAS and run commands directly. NOTE: The commands listed here uses the --rm flag which will remove the container from the host when it shuts down. And as we're not mounting any volumes here, your host system will not be altered from running any of these commands. If any command breaks with this principle it will be noted.","title":"Introduction and assumptions"},{"location":"debug/#checking_that_docker_works_properly","text":"In order for this container to work you have to have a working Docker installation on your host. We'll begin very simple with this command that will print a welcome message if Docker is properly installed. docker run --rm hello-world Then we can try to run an alpine image, install curl and run curl to get your public IP. This verifies that Docker containers on your host has a working internet access and that they can look up hostnames with DNS. docker run --rm -it alpine sh -c \"apk add curl && curl ipecho.net/plain\" If you get an error with \"Could not resolve host\" then you have to look at the dns options in the Docker run reference . Finally we will check that your Docker daemon runs with a bridge network as the default network driver. The following command runs an alpine container and prints it's iptable routes. It probably outputs two lines and one of them starts with 172.x.0.0/16 dev eth0 and the other one also references 172.x.0.1 . The 172 addresses are a sign that you're on a Docker bridge network. If your local IP like 192.168.x.y shows up your container is running with host networking and the VPN container would affect the entire host instead of just affecting Transmission running within the container. docker run --rm -it alpine ip r If you have gotten any errors so far you have to refer to Docker documentation and other forums to get help getting Docker to work.","title":"Checking that Docker works properly"},{"location":"debug/#try_running_the_container_with_an_invalid_setup","text":"We'll keep this brief because it's not the most useful step, but you can actually verify a bit anyways. Run this command (even if PIA is not your provider) and do not insert your real username/password: docker run --rm -it -e OPENVPN_PROVIDER=PIA -e OPENVPN_CONFIG=france -e OPENVPN_USERNAME=donald -e OPENVPN_PASSWORD=duck haugene/transmission-openvpn At this point the commands are getting longer. I'll start breaking them up into lines using \\ to escape the line breaks. For those that are new to shell commands; a \\ at the end of the line will tell the shell to keep on reading as if it was on the same line. You can copy-paste this somewhere and put everythin on the same line and remove the \\ characters if you want to. The same command then becomes: docker run --rm -it \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=donald \\ -e OPENVPN_PASSWORD=duck \\ haugene/transmission-openvpn This command should fail and exit with three lines that look like this (I've trimmed it a bit): Peer Connection Initiated with ... AUTH: Received control message: AUTH_FAILED SIGTERM[soft,auth-failure] received, process exiting And this is not nothing. The container has made contact with the VPN server and they have agreed that you do not have provided correct authentication. So we're getting somewhere.","title":"Try running the container with an invalid setup"},{"location":"debug/#running_with_a_valid_configuration","text":"Upgrading slightly from the last command we need to add our real provider, config and username/password. This is what the container needs to be able to connect to VPN. The config is not a mandatory option and all providers should have a default that is used if you don't set it. But I will set it here anyways as I think it's good to know where and what you're connecting to. The command is basically the same. I'm going to stick with PIA/france as I am a PIA user, but you should set one of the supported providers or provide your own config using the custom configuration option . Since I'm now expecting to connect successfully to my VPN provider I have to give the container elevated access to modify networking needed to establish a VPN tunnel. I'll add the --cap-add=NET_ADMIN and you can read more about that here . Also because I'm using PIA and they support port forwarding which is automatically configured in this container I will disable that script for now. It's unnecessary at this point and I don't want to introduce more error sources than I have to. docker run --rm -it --cap-add=NET_ADMIN \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=username \\ -e OPENVPN_PASSWORD=password \\ -e DISABLE_PORT_UPDATER=true \\ haugene/transmission-openvpn The logs should be longer this time, and you should end up with Initialization Sequence Completed at the end. If you don't see this message then look through your logs for errors and see if you can find your error in the list in the FAQ . If the error is not easily understandable or listed in the FAQ, open a new issue on it.","title":"Running with a valid configuration"},{"location":"debug/#checking_if_transmission_is_running","text":"We're continuing with the docker run command from the last example. Because we have not yet introduced the LOCAL_NETWORK variable you cannot access Transmission that is running in the container. We have not exposed any ports on your host either so Transmission is not reachable from outside of the container as of now. You don't need to expose Transmission outside of the container to contact it though. You can get another shell inside the same container that you are running and try to curl Transmission web ui from there. If this gets too complicated then you can skip to the next point, but please try to come back here if the next point fails. One thing is not being able to access Transmission which might be network related, but if Transmission is not running it's something else entirely. If you run docker ps you should get a list of all running docker containers. If the list is too long then you can ask for only transmission-openvpn containers with docker ps --filter ancestor=haugene/transmission-openvpn . Using the Container ID from that list you can run docker exec -it <container-id> bash . Once you're in the container run curl localhost:9091 and you should expect to get <h1>301: Moved Permanently</h1> in return. This is because Transmission runs at /web/transmission and tries to redirect you there. It doesn't matter because you now see that Transmission is running and apparently doing well.","title":"Checking if Transmission is running"},{"location":"debug/#accessing_transmission_web_ui","text":"If you've come this far we hopefully will be able to connect to the Transmission Web UI from your browser. In order to do this we have to know what LAN IP your system is on. The reason for this is a bit complex and is described in the VPN networking section. The short version is that OpenVPN need to be able to differentiate between what traffic to tunnel and what to let go. Since the VPN is running on the Docker bridge network it is not able to detect computers on your LAN as actually being local devices. We'll base ourselves on the command from the previous sections, but to access Transmission we need to expose the 9091 port to the host and tell the containers what IP ranges NOT to tunnel. Whatever you put in LOCAL_NETWORK will be trusted as a local network and traffic to those IPs will not be tunneled. Here we will assume that you're on one of the common 192.168.x.y subnets. The command then becomes: docker run --rm -it --cap-add=NET_ADMIN \\ -p 9091:9091 \\ -e LOCAL_NETWORK=192.168.0.0/16 \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=username \\ -e OPENVPN_PASSWORD=password \\ -e DISABLE_PORT_UPDATER=true \\ haugene/transmission-openvpn With any luck you should now be able to access Transmission at http://localhost:9091 or whatever server IP where you have started the container. NOTE: If you're trying to run this beside another container you can use -p 9092:9091 to bind 9092 on the host instead of 9091 and avoid port conflict.","title":"Accessing Transmission Web UI"},{"location":"debug/#now_what","text":"If this guide has failed at some point then you should create an issue for it. Please add the command that you ran and the logs that was produced. If you're now able to access Transmission and it seems to work correctly then you should add a volume mount to the /data folder in the container. You'll then have a setup like what's shown on the main GitHub page of this project. If you have another setup that does not work then you now have two versions to compare and maybe that will lead you to find the error in your old setup. If the setup is the same but this version works then the error is in your state. Transmission stores its state in /data/transmission-home by default and it might have gotten corrupt. One simple thing to try is to delete the settings.json file that is found here. We do mess with that file and we might have corrupted it. Apart from that we do not change anything within the Transmission folder and any issues should be asked in Transmission forums.","title":"Now what?"},{"location":"debug/#conclusion","text":"I hope this has helped you to solve your problem or at least narrow down where it's coming from. If you have suggestions for improvements do not hesitate to create an issue or even better open a PR with your proposed changes.","title":"Conclusion"},{"location":"faq/","text":"The container runs, but I can't access the web ui How do I enable authentication in the web ui How do I verify that my traffic is using VPN RTNETLINK answers: File exists RTNETLINK answers: Invalid argument TUNSETIFF tun: Operation not permitted Error resolving host address Container loses connection after some time Set the ping-exit option for OpenVPN and restart-flag in Docker Use a third party tool to monitor and restart the container AUTH: Received control message: AUTH_FAILED The container runs, but I can't access the web ui \u00b6 TODO : Short explanation and link to networking How do I enable authentication in the web ui \u00b6 You can do this either by setting the appropriate fields in settings.json which is found in TRANSMISSION_HOME which defaults to /data/transmission-home so it will be available on your host where you mount the /data volume. Remember that Transmission overwrites the config when it shuts down, so do this when the container is not running. Or you can set it using the convenience environment variables. They will then override the settings on every container startup. The environment variables you have to set are: TRANSMISSION_RPC_USERNAME=username TRANSMISSION_RPC_PASSWORD=password TRANSMISSION_RPC_AUTHENTICATION_REQUIRED=true PS: Be cautious of special characters in the username or password. We've had multiple errors with that and have not provided a fix yet. Escaping special characters could be an option, but the easiest solution is just to avoid them. Make the password longer instead ;) Or write it into settings.json manually as first described. How do I verify that my traffic is using VPN \u00b6 There are many ways of doing this, and I welcome you to add to this list if you have any suggestsions. You can exec into the container and throug the shell use curl to ask for your public IP. There are multiple endpoints for this but here are a few suggestions: curl http://ipinfo.io/ip curl http://ipecho.net/plain curl icanhazip.com Or you could use a test torrent service to download a torrent file and then you can get the IP from that tracker. http://ipmagnet.services.cbcdn.com/ https://torguard.net/checkmytorrentipaddress.php RTNETLINK answers: File exists \u00b6 TODO : Conflicting LOCAL_NETWORK values. Short explanation and link to networking RTNETLINK answers: Invalid argument \u00b6 This can occur because you have specified an invalid subnet or possibly specified an IP Address in CIDR format instead of a subnet. Your LOCAL_NETWORK property must be aimed at a subnet and not at an IP Address. A valid example would be ``` LOCAL_NETWORK=10.80.0.0/24 ``` but an invalid target route that would cause this error might be ``` #Invalid because the subnet for this range would be 10.20.30.0/24 LOCAL_NETWORK=10.20.30.45/24 ``` To check your value, you can use a subnet calculator . * Enter your IP Address - the portion before the mask, 10.20.30.45 here * select the subnet that matches - the /24 portion here * Take the Network Address that is returned - 10.20.30.0 in this case TUNSETIFF tun: Operation not permitted \u00b6 This is usually a question of permissions. Have you set the NET_ADMIN capabilities to the container? What if you use docker run --privileged , do you still get that error? This is an error where we haven't got too much information. If the hints above get you nowhere, create an issue. Error resolving host address \u00b6 This error can happen multiple places in the scripts. The most common is that it happens with curl trying to download the latest .ovpn config bundle for those providers that has an update script, or that OpenVPN throws the error when trying to connect to the VPN server. The curl error looks something like curl: (6) Could not resolve host: ... and OpenVPN says RESOLVE: Cannot resolve host address: ... . Either way the problem is that your container does not have a valid DNS setup. We have two recommended ways of addressing this. The first solution is to use the dns option offered by Docker. This is available in Docker run as well as Docker Compose . You can add --dns 8.8.8.8 --dns 8.8.4.4 to use Google DNS servers. Or add the corresponding block in docker-compose.yml: dns: - 8.8.8.8 - 8.8.4.4 You can of course use any DNS servers you want here. Google servers are popular. So is Cloudflare DNS. The second approach is to use some environment variables to override /etc/resolv.conf in the container. Using the same DNS servers as in the previous approach you can set: OVERRIDE_DNS_1=8.8.8.8 OVERRIDE_DNS_2=8.8.4.4 This will be read by the startup script and it will override the contents of /etc/resolv.conf accordingly. You can have one or more of these servers and they will be sorted alphabetically. What is the difference between these solutions? A good question as they both seem to override what DNS servers the container should use. However they are not equal. The first solution uses the dns flags from Docker. This will mean that we instruct Docker to use these DNS servers for the container, but the resolv.conf file in the container will still point to the Docker DNS service. Docker might have many reasons for this but one of them is at least for service discovery. If you're running your container as a part of a larger docker-compose file or custom docker network and you want to be able to lookup the other containers based on their service names then you need to use the Docker DNS service. By using the --dns flags you should have both control of what DNS servers are used for external requests as well as container DNS lookup. The second solution is more direct. It rewrites the resolv.conf file so that it no longer refers to the Docker DNS service. The effects of this is that you lose Docker service discovery from the container (other containers in the same network can still resolve it) but you have cut out a middleman and potential point of error. I'm not sure why this some times is necessary but it has proven to fix the issue in some cases. A possible third option If you're facing the OpenVPN error (not curl) then your provider might have config files with IP addresses instead of DNS. That way your container won't need DNS to do a lookup for the server. Note that the trackers will still need DNS, so this will only solve the problem for you if it is your local network that in some way is blocking the DNS. Container loses connection after some time \u00b6 For some users, on some platforms, apparently this is an issue. I have not encountered this myself - but there is no doubt that it's recurring. Why does the container lose connectivity? That we don't know and it could be many different reasons that manifest the same symptoms. We do however have some possible solutions. Set the ping-exit option for OpenVPN and restart-flag in Docker \u00b6 Most provider configs have a ping-restart option set. So if the tunnel fails, OpenVPN will restart and re-connect. That works well on regular systems. The problem is that if the container has lost internet connection restarting OpenVPN will not fix anything. What you can do though is to set/override this option using OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60 . This will tell OpenVPN to exit when it cannot ping the server for 1 minute. When OpenVPN exits, the container will exit. And if you've then set restart=always or restart=unless-stopped in your Docker config then Docker will restart the container and that could/should restore connectivity. VPN providers sometime push options to their clients after they connect. This is visible in the logs if they do. If they push ping-restart that can override your settings. So you could consider adding --pull-filter ignore ping to the options above. This approach will probably work, especially if you're seeing logs like these from before: Inactivity timeout (--ping-restart), restarting SIGUSR1[soft,ping-restart] received, process restarting Use a third party tool to monitor and restart the container \u00b6 The container has a health check script that is run periodically. It will report the health status to Docker and the container will show as \"unhealthy\" if basic network connectivity is broken. You can write your own script and add it to cron, or you can use a tool like https://github.com/willfarrell/docker-autoheal to look for and restart unhealthy containers. This container has the autoheal label by default so it is compatible with the willfarrell/autoheal image AUTH: Received control message: AUTH_FAILED \u00b6 If your logs end like this, the wrong username/password was sent to your VPN provider. AUTH: Received control message: AUTH_FAILED SIGTERM[soft,auth-failure] received, process exiting We can divide the possible errors here into three. You have entered the wrong credentials, the server has some kind of error or the container has messed up your credentials. We have had challenges with special characters. Having \"?= as part of your password has tripped up our scripts from time to time. NOTE Some providers have multiple sets of credentials. Some for OpenVPN, others for web login, proxy solutions, etc. Make sure that you use the ones intended for OpenVPN. PIA users: this has recently changed. It used to be a separate pair, but now you should use the same login as you do in the web control panel. Before you were supposed to use a username like x12345, now its the p12345 one. There is also a 99 character limit on password length. First check that your credentials are correct. Some providers have separate credentials for OpenVPN so it might not be the same as for their apps. Secondly, test a few different servers just to make sure that it's not just a faulty server. If this doesn't resolve it, it's probably the container. To verify this you can mount a volume to /config in the container. So for example /temporary/folder:/config . Your credentials will be written to /config/openvpn-credentials.txt when the container starts, more on that here . So by mounting this folder you will be able to check the contents of that text file. The first line should be your username, the second should be your password. This file is what's passed to OpenVPN. If your username/password is correct here then you should probably contact your provider.","title":"Frequently asked questions"},{"location":"faq/#the_container_runs_but_i_cant_access_the_web_ui","text":"TODO : Short explanation and link to networking","title":"The container runs, but I can't access the web ui"},{"location":"faq/#how_do_i_enable_authentication_in_the_web_ui","text":"You can do this either by setting the appropriate fields in settings.json which is found in TRANSMISSION_HOME which defaults to /data/transmission-home so it will be available on your host where you mount the /data volume. Remember that Transmission overwrites the config when it shuts down, so do this when the container is not running. Or you can set it using the convenience environment variables. They will then override the settings on every container startup. The environment variables you have to set are: TRANSMISSION_RPC_USERNAME=username TRANSMISSION_RPC_PASSWORD=password TRANSMISSION_RPC_AUTHENTICATION_REQUIRED=true PS: Be cautious of special characters in the username or password. We've had multiple errors with that and have not provided a fix yet. Escaping special characters could be an option, but the easiest solution is just to avoid them. Make the password longer instead ;) Or write it into settings.json manually as first described.","title":"How do I enable authentication in the web ui"},{"location":"faq/#how_do_i_verify_that_my_traffic_is_using_vpn","text":"There are many ways of doing this, and I welcome you to add to this list if you have any suggestsions. You can exec into the container and throug the shell use curl to ask for your public IP. There are multiple endpoints for this but here are a few suggestions: curl http://ipinfo.io/ip curl http://ipecho.net/plain curl icanhazip.com Or you could use a test torrent service to download a torrent file and then you can get the IP from that tracker. http://ipmagnet.services.cbcdn.com/ https://torguard.net/checkmytorrentipaddress.php","title":"How do I verify that my traffic is using VPN"},{"location":"faq/#rtnetlink_answers_file_exists","text":"TODO : Conflicting LOCAL_NETWORK values. Short explanation and link to networking","title":"RTNETLINK answers: File exists"},{"location":"faq/#rtnetlink_answers_invalid_argument","text":"This can occur because you have specified an invalid subnet or possibly specified an IP Address in CIDR format instead of a subnet. Your LOCAL_NETWORK property must be aimed at a subnet and not at an IP Address. A valid example would be ``` LOCAL_NETWORK=10.80.0.0/24 ``` but an invalid target route that would cause this error might be ``` #Invalid because the subnet for this range would be 10.20.30.0/24 LOCAL_NETWORK=10.20.30.45/24 ``` To check your value, you can use a subnet calculator . * Enter your IP Address - the portion before the mask, 10.20.30.45 here * select the subnet that matches - the /24 portion here * Take the Network Address that is returned - 10.20.30.0 in this case","title":"RTNETLINK answers: Invalid argument"},{"location":"faq/#tunsetiff_tun_operation_not_permitted","text":"This is usually a question of permissions. Have you set the NET_ADMIN capabilities to the container? What if you use docker run --privileged , do you still get that error? This is an error where we haven't got too much information. If the hints above get you nowhere, create an issue.","title":"TUNSETIFF tun: Operation not permitted"},{"location":"faq/#error_resolving_host_address","text":"This error can happen multiple places in the scripts. The most common is that it happens with curl trying to download the latest .ovpn config bundle for those providers that has an update script, or that OpenVPN throws the error when trying to connect to the VPN server. The curl error looks something like curl: (6) Could not resolve host: ... and OpenVPN says RESOLVE: Cannot resolve host address: ... . Either way the problem is that your container does not have a valid DNS setup. We have two recommended ways of addressing this. The first solution is to use the dns option offered by Docker. This is available in Docker run as well as Docker Compose . You can add --dns 8.8.8.8 --dns 8.8.4.4 to use Google DNS servers. Or add the corresponding block in docker-compose.yml: dns: - 8.8.8.8 - 8.8.4.4 You can of course use any DNS servers you want here. Google servers are popular. So is Cloudflare DNS. The second approach is to use some environment variables to override /etc/resolv.conf in the container. Using the same DNS servers as in the previous approach you can set: OVERRIDE_DNS_1=8.8.8.8 OVERRIDE_DNS_2=8.8.4.4 This will be read by the startup script and it will override the contents of /etc/resolv.conf accordingly. You can have one or more of these servers and they will be sorted alphabetically. What is the difference between these solutions? A good question as they both seem to override what DNS servers the container should use. However they are not equal. The first solution uses the dns flags from Docker. This will mean that we instruct Docker to use these DNS servers for the container, but the resolv.conf file in the container will still point to the Docker DNS service. Docker might have many reasons for this but one of them is at least for service discovery. If you're running your container as a part of a larger docker-compose file or custom docker network and you want to be able to lookup the other containers based on their service names then you need to use the Docker DNS service. By using the --dns flags you should have both control of what DNS servers are used for external requests as well as container DNS lookup. The second solution is more direct. It rewrites the resolv.conf file so that it no longer refers to the Docker DNS service. The effects of this is that you lose Docker service discovery from the container (other containers in the same network can still resolve it) but you have cut out a middleman and potential point of error. I'm not sure why this some times is necessary but it has proven to fix the issue in some cases. A possible third option If you're facing the OpenVPN error (not curl) then your provider might have config files with IP addresses instead of DNS. That way your container won't need DNS to do a lookup for the server. Note that the trackers will still need DNS, so this will only solve the problem for you if it is your local network that in some way is blocking the DNS.","title":"Error resolving host address"},{"location":"faq/#container_loses_connection_after_some_time","text":"For some users, on some platforms, apparently this is an issue. I have not encountered this myself - but there is no doubt that it's recurring. Why does the container lose connectivity? That we don't know and it could be many different reasons that manifest the same symptoms. We do however have some possible solutions.","title":"Container loses connection after some time"},{"location":"faq/#set_the_ping-exit_option_for_openvpn_and_restart-flag_in_docker","text":"Most provider configs have a ping-restart option set. So if the tunnel fails, OpenVPN will restart and re-connect. That works well on regular systems. The problem is that if the container has lost internet connection restarting OpenVPN will not fix anything. What you can do though is to set/override this option using OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60 . This will tell OpenVPN to exit when it cannot ping the server for 1 minute. When OpenVPN exits, the container will exit. And if you've then set restart=always or restart=unless-stopped in your Docker config then Docker will restart the container and that could/should restore connectivity. VPN providers sometime push options to their clients after they connect. This is visible in the logs if they do. If they push ping-restart that can override your settings. So you could consider adding --pull-filter ignore ping to the options above. This approach will probably work, especially if you're seeing logs like these from before: Inactivity timeout (--ping-restart), restarting SIGUSR1[soft,ping-restart] received, process restarting","title":"Set the ping-exit option for OpenVPN and restart-flag in Docker"},{"location":"faq/#use_a_third_party_tool_to_monitor_and_restart_the_container","text":"The container has a health check script that is run periodically. It will report the health status to Docker and the container will show as \"unhealthy\" if basic network connectivity is broken. You can write your own script and add it to cron, or you can use a tool like https://github.com/willfarrell/docker-autoheal to look for and restart unhealthy containers. This container has the autoheal label by default so it is compatible with the willfarrell/autoheal image","title":"Use a third party tool to monitor and restart the container"},{"location":"faq/#auth_received_control_message_auth_failed","text":"If your logs end like this, the wrong username/password was sent to your VPN provider. AUTH: Received control message: AUTH_FAILED SIGTERM[soft,auth-failure] received, process exiting We can divide the possible errors here into three. You have entered the wrong credentials, the server has some kind of error or the container has messed up your credentials. We have had challenges with special characters. Having \"?= as part of your password has tripped up our scripts from time to time. NOTE Some providers have multiple sets of credentials. Some for OpenVPN, others for web login, proxy solutions, etc. Make sure that you use the ones intended for OpenVPN. PIA users: this has recently changed. It used to be a separate pair, but now you should use the same login as you do in the web control panel. Before you were supposed to use a username like x12345, now its the p12345 one. There is also a 99 character limit on password length. First check that your credentials are correct. Some providers have separate credentials for OpenVPN so it might not be the same as for their apps. Secondly, test a few different servers just to make sure that it's not just a faulty server. If this doesn't resolve it, it's probably the container. To verify this you can mount a volume to /config in the container. So for example /temporary/folder:/config . Your credentials will be written to /config/openvpn-credentials.txt when the container starts, more on that here . So by mounting this folder you will be able to check the contents of that text file. The first line should be your username, the second should be your password. This file is what's passed to OpenVPN. If your username/password is correct here then you should probably contact your provider.","title":"AUTH: Received control message: AUTH_FAILED"},{"location":"provider-specific/","text":"COMING SOON \u00b6 NOTE: This page is just moved from it's previous location. A re-write is coming. I'm on it (#1558) NORDVPN \u00b6 The update script is based on the NordVPN API. The API sends back the best recommended OpenVPN configuration file based on the filters given. Available ENV variables in the container to define via the NordVPN API the file to use are: Variable Function Example NORDVPN_COUNTRY Two character country code. See /servers/countries for full list. NORDVPN_COUNTRY=US NORDVPN_CATEGORY Server type (P2P, Standard, etc). See /servers/groups for full list. Use either title or identifier from the list. NORDVPN_CATEGORY=legacy_p2p NORDVPN_PROTOCOL Either tcp or udp . (values identifier more available at https://api.nordvpn.com/v1/technologies, may need script adaptation) NORDVPN_PROTOCOL=tcp The file is then downloaded using the API to find the best server according to the variables, here an albanian, using tcp: selecting server (limit answer to 1): [ANSWER]= https://api.nordvpn.com/v1/servers/recommendations?filters[country_id]=2&filters[servers_technologies][identifier]=openvpn_tcp&filters[servers_group][identifier]=legacy_group_category&limit=1 download selected server's config: https://downloads.nordcdn.com/configs/files/ovpn_[NORDVPN_PROTOCOL]/servers/[ANSWER.0.HOSTNAME][] => https://downloads.nordcdn.com/configs/files/ovpn_tcp/servers/al9.nordvpn.com.tcp.ovpn A possible evolution would be to check server's load to select the most available one. limit numbers of returned server to 10 use https://api.nordvpn.com/server/stats to collect cpu's load select the more available server.","title":"Provider specific settings"},{"location":"provider-specific/#coming_soon","text":"NOTE: This page is just moved from it's previous location. A re-write is coming. I'm on it (#1558)","title":"COMING SOON"},{"location":"provider-specific/#nordvpn","text":"The update script is based on the NordVPN API. The API sends back the best recommended OpenVPN configuration file based on the filters given. Available ENV variables in the container to define via the NordVPN API the file to use are: Variable Function Example NORDVPN_COUNTRY Two character country code. See /servers/countries for full list. NORDVPN_COUNTRY=US NORDVPN_CATEGORY Server type (P2P, Standard, etc). See /servers/groups for full list. Use either title or identifier from the list. NORDVPN_CATEGORY=legacy_p2p NORDVPN_PROTOCOL Either tcp or udp . (values identifier more available at https://api.nordvpn.com/v1/technologies, may need script adaptation) NORDVPN_PROTOCOL=tcp The file is then downloaded using the API to find the best server according to the variables, here an albanian, using tcp: selecting server (limit answer to 1): [ANSWER]= https://api.nordvpn.com/v1/servers/recommendations?filters[country_id]=2&filters[servers_technologies][identifier]=openvpn_tcp&filters[servers_group][identifier]=legacy_group_category&limit=1 download selected server's config: https://downloads.nordcdn.com/configs/files/ovpn_[NORDVPN_PROTOCOL]/servers/[ANSWER.0.HOSTNAME][] => https://downloads.nordcdn.com/configs/files/ovpn_tcp/servers/al9.nordvpn.com.tcp.ovpn A possible evolution would be to check server's load to select the most available one. limit numbers of returned server to 10 use https://api.nordvpn.com/server/stats to collect cpu's load select the more available server.","title":"NORDVPN"},{"location":"rss-plugin/","text":"The Transmission RSS plugin can optionally be run as a separate container. It allows downloading torrents based on an RSS URL, see Plugin page . $ docker run -d \\ -e \"RSS_URL=<URL>\" \\ --link <transmission-container>:transmission \\ --name \"transmission-rss\" \\ haugene/transmission-rss","title":"RSS plugin"},{"location":"run-container/","text":"Running the container \u00b6 Many platforms ship with a Docker runtime and have their own way of setting this up. I'm then thinking about NAS servers specifically, but also Unraid and others. In addition to this we have the container management solutions like Portainer This page will only discuss the tooling that a Docker installation comes with. That means docker run .. and docker-compose . In the end that is what the other managers do as well and it's the common ground here. I'm very happy to set up a platform specific installation page and link to it from here. Open an issue or PR if you want to contribute with documentation for your favourite platform. The images available on the Docker Hub are multiarch manifests. This means that they point to multiple images that are built for different CPU architectures like ARM for Raspberry Pi. You can haugene/transmission-openvpn on any of these architectures and Docker will get the correct one. Starting the container \u00b6 The example Docker run command looks like this: $ docker run --cap-add=NET_ADMIN -d \\ -v /your/storage/path/:/data \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=user \\ -e OPENVPN_PASSWORD=pass \\ -e LOCAL_NETWORK=192.168.0.0/16 \\ --log-driver json-file \\ --log-opt max-size=10m \\ -p 9091:9091 \\ haugene/transmission-openvpn The example docker-compose.yml looks like this: version: '3.3' services: transmission-openvpn: cap_add: - NET_ADMIN volumes: - '/your/storage/path/:/data' environment: - OPENVPN_PROVIDER=PIA - OPENVPN_CONFIG=france - OPENVPN_USERNAME=user - OPENVPN_PASSWORD=pass - LOCAL_NETWORK=192.168.0.0/16 logging: driver: json-file options: max-size: 10m ports: - '9091:9091' image: haugene/transmission-openvpn These configs are equivalent. Running docker-compose up with that compose file will result in the same options being sent to the Docker engine as the run statement before it. Three things to remember \u00b6 1. The container assumes that you mount a folder to /data \u00b6 Technically you don't have to do this, but it is by far the most manageable way of getting the downloaded files onto your host system and Transmission will store it's state there. So if you don't mount this directory then you will loose all your torrents on image updates. 2. It is not mandatory, but setting OPENVPN_CONFIG is good \u00b6 If you don't set this then there should be a default config for each provider that is chosen, and that should work fine. The benefit of choosing yourself is that you can choose a region that is closer to you and that might be better for speed. I also believe that tinkering with this builds some familiarity with the image and some confidence and understanding for future debugging. We're now moving towards a setup where we download the configs for our providers when the container starts. That is great from a maintenance perspective, but it also means that we don't know the valid choices for the providers ahead of time. A tip for finding out is to set OPENVPN_CONFIG=dummy and start it. This will fail, but in the logs it will print all the valid options. Pro tip: choose multiple servers. For example: OPENVPN_CONFIG=france,sweden,austria,italy,belgium This will ensure a location near you, but at the same time it will allow some redundancy. Set Docker to restart the container automatically and you have a failover mechanism. The container chooses one of the configs at random when it starts and it will bounce from server to server until it finds one that works. 3. You might not be able to access the Web UI on the first try \u00b6 The LOCAL_NETWORK=192.168.0.0/16 tries to fix this for you, but it might not work if your local LAN DHCP server hands out addresses outside that range. If your local network is in the 10.x.y.z space for example then you need to set LOCAL_NETWORK=10.x.0.0/16 or LOCAL_NETWORK=10.x.y.0/24 . These are called CIDR addresses and you can read up on them. The short story is that /24 will allow for any value in the last digit place while /16 will allow any value in the two last places. Be sure to only allow IPs that are in the private IP ranges . This option punches a hole in the VPN for the IPs that you specify. It is neccessary to reach your Web UI but narrower ranges are better than wide ones. With that said. If you know that you're on a \"typical\" network with your router at 192.168.1.1, then LOCAL_NETWORK=192.168.1.0/24 is better than LOCAL_NETWORK=192.168.0.0/16 . That way you only allow access form 192.168.1.x instead of 192.168.x.y. There is an alternative to the LOCAL_NETWORK environment variable, and that is a reverse proxy in the same docker network as the vpn container. Because this topic is both quite complex and very important there is a separate page on VPN and Networking in the container and it goes into depth on why this is.","title":"Running the container"},{"location":"run-container/#running_the_container","text":"Many platforms ship with a Docker runtime and have their own way of setting this up. I'm then thinking about NAS servers specifically, but also Unraid and others. In addition to this we have the container management solutions like Portainer This page will only discuss the tooling that a Docker installation comes with. That means docker run .. and docker-compose . In the end that is what the other managers do as well and it's the common ground here. I'm very happy to set up a platform specific installation page and link to it from here. Open an issue or PR if you want to contribute with documentation for your favourite platform. The images available on the Docker Hub are multiarch manifests. This means that they point to multiple images that are built for different CPU architectures like ARM for Raspberry Pi. You can haugene/transmission-openvpn on any of these architectures and Docker will get the correct one.","title":"Running the container"},{"location":"run-container/#starting_the_container","text":"The example Docker run command looks like this: $ docker run --cap-add=NET_ADMIN -d \\ -v /your/storage/path/:/data \\ -e OPENVPN_PROVIDER=PIA \\ -e OPENVPN_CONFIG=france \\ -e OPENVPN_USERNAME=user \\ -e OPENVPN_PASSWORD=pass \\ -e LOCAL_NETWORK=192.168.0.0/16 \\ --log-driver json-file \\ --log-opt max-size=10m \\ -p 9091:9091 \\ haugene/transmission-openvpn The example docker-compose.yml looks like this: version: '3.3' services: transmission-openvpn: cap_add: - NET_ADMIN volumes: - '/your/storage/path/:/data' environment: - OPENVPN_PROVIDER=PIA - OPENVPN_CONFIG=france - OPENVPN_USERNAME=user - OPENVPN_PASSWORD=pass - LOCAL_NETWORK=192.168.0.0/16 logging: driver: json-file options: max-size: 10m ports: - '9091:9091' image: haugene/transmission-openvpn These configs are equivalent. Running docker-compose up with that compose file will result in the same options being sent to the Docker engine as the run statement before it.","title":"Starting the container"},{"location":"run-container/#three_things_to_remember","text":"","title":"Three things to remember"},{"location":"run-container/#1_the_container_assumes_that_you_mount_a_folder_to_data","text":"Technically you don't have to do this, but it is by far the most manageable way of getting the downloaded files onto your host system and Transmission will store it's state there. So if you don't mount this directory then you will loose all your torrents on image updates.","title":"1. The container assumes that you mount a folder to /data"},{"location":"run-container/#2_it_is_not_mandatory_but_setting_openvpn_config_is_good","text":"If you don't set this then there should be a default config for each provider that is chosen, and that should work fine. The benefit of choosing yourself is that you can choose a region that is closer to you and that might be better for speed. I also believe that tinkering with this builds some familiarity with the image and some confidence and understanding for future debugging. We're now moving towards a setup where we download the configs for our providers when the container starts. That is great from a maintenance perspective, but it also means that we don't know the valid choices for the providers ahead of time. A tip for finding out is to set OPENVPN_CONFIG=dummy and start it. This will fail, but in the logs it will print all the valid options. Pro tip: choose multiple servers. For example: OPENVPN_CONFIG=france,sweden,austria,italy,belgium This will ensure a location near you, but at the same time it will allow some redundancy. Set Docker to restart the container automatically and you have a failover mechanism. The container chooses one of the configs at random when it starts and it will bounce from server to server until it finds one that works.","title":"2. It is not mandatory, but setting OPENVPN_CONFIG is good"},{"location":"run-container/#3_you_might_not_be_able_to_access_the_web_ui_on_the_first_try","text":"The LOCAL_NETWORK=192.168.0.0/16 tries to fix this for you, but it might not work if your local LAN DHCP server hands out addresses outside that range. If your local network is in the 10.x.y.z space for example then you need to set LOCAL_NETWORK=10.x.0.0/16 or LOCAL_NETWORK=10.x.y.0/24 . These are called CIDR addresses and you can read up on them. The short story is that /24 will allow for any value in the last digit place while /16 will allow any value in the two last places. Be sure to only allow IPs that are in the private IP ranges . This option punches a hole in the VPN for the IPs that you specify. It is neccessary to reach your Web UI but narrower ranges are better than wide ones. With that said. If you know that you're on a \"typical\" network with your router at 192.168.1.1, then LOCAL_NETWORK=192.168.1.0/24 is better than LOCAL_NETWORK=192.168.0.0/16 . That way you only allow access form 192.168.1.x instead of 192.168.x.y. There is an alternative to the LOCAL_NETWORK environment variable, and that is a reverse proxy in the same docker network as the vpn container. Because this topic is both quite complex and very important there is a separate page on VPN and Networking in the container and it goes into depth on why this is.","title":"3. You might not be able to access the Web UI on the first try"},{"location":"supported-providers/","text":"This is a list of providers that are bundled within the image. Feel free to create an issue if your provider is not on the list, but keep in mind that some providers generate config files per user. This means that your login credentials are part of the config an can therefore not be bundled. In this case you can use the custom provider setup described later in this readme. The custom provider setting can be used with any provider. Provider Name Config Value ( OPENVPN_PROVIDER ) Anonine ANONINE AnonVPN ANONVPN BlackVPN BLACKVPN BTGuard BTGUARD Cryptostorm CRYPTOSTORM ExpressVPN EXPRESSVPN FastestVPN FASTESTVPN FreeVPN FREEVPN FrootVPN FROOT FrostVPN FROSTVPN GhostPath GHOSTPATH Giganews GIGANEWS HideMe HIDEME HideMyAss HIDEMYASS IntegrityVPN INTEGRITYVPN IPVanish IPVANISH IronSocket IRONSOCKET Ivacy IVACY IVPN IVPN Mullvad MULLVAD NordVPN NORDVPN OctaneVPN OCTANEVPN OVPN OVPN Perfect Privacy PERFECTPRIVACY Private Internet Access PIA Privado PRIVADO PrivateVPN PRIVATEVPN ProtonVPN PROTONVPN proXPN PROXPN PureVPN PUREVPN RA4W VPN RA4W SaferVPN SAFERVPN SlickVPN SLICKVPN Smart DNS Proxy SMARTDNSPROXY SmartVPN SMARTVPN Surfshark SURFSHARK TigerVPN TIGER TorGuard TORGUARD Trust.Zone TRUSTZONE TunnelBear TUNNELBEAR VPNArea.com VPNAREA VPNBook.com VPNBOOK VPNFacile VPNFACILE VPNTunnel VPNTUNNEL VPNUnlimited VPNUNLIMITED VPN.AC VPNAC VPN.ht VPNHT VyprVpn VYPRVPN Windscribe WINDSCRIBE ZoogVPN ZOOGVPN Adding new providers \u00b6 If your VPN provider is not in the list of supported providers you could always create an issue on GitHub and see if someone could add it for you. But if you're feeling up for doing it yourself, here's a couple of pointers. You clone this repository and create a new folder under \"openvpn\" where you put the .ovpn files your provider gives you. Depending on the structure of these files you need to make some adjustments. For example if they come with a ca.crt file that is referenced in the config you need to update this reference to the path it will have inside the container (which is /etc/openvpn/...). You also have to set where to look for your username/password. There is a script called adjustConfigs.sh that could help you. After putting your .ovpn files in a folder, run that script with your folder name as parameter and it will try to do the changes described above. If you use it or not, reading it might give you some help in what you're looking to change in the .ovpn files. Once you've finished modifying configs, you build the container and run it with OPENVPN_PROVIDER set to the name of the folder of configs you just created (it will be lowercased to match the folder names). And that should be it! So, you've just added your own provider and you're feeling pretty good about it! Why don't you fork this repository, commit and push your changes and submit a pull request? Share your provider with the rest of us! :) Please submit your PR to the dev branch in that case. Using a custom provider \u00b6 If you want to run the image with your own provider without building a new image, that is also possible. For some providers, like AirVPN, the .ovpn files are generated per user and contains credentials. They should not be added to a public image. This is what you do: Add a new volume mount to your docker run command that mounts your config file: -v /path/to/your/config.ovpn:/etc/openvpn/custom/default.ovpn Then you can set OPENVPN_PROVIDER=CUSTOM and the container will use the config you provided. NOTE: Your .ovpn config file probably contains a line that says auth-user-pass . This will prompt OpenVPN to ask for the username and password. As this is running in a scripted environment that is not possible. Change it for auth-user-pass /config/openvpn-credentials.txt which is the file where your OPENVPN_USERNAME and OPENVPN_PASSWORD variables will be written to. If you are using AirVPN or other provider with credentials in the config file, you still need to set OPENVPN_USERNAME and OPENVPN_PASSWORD as this is required by the startup script. They will not be read by the .ovpn file, so you can set them to whatever. Note that you still need to modify your .ovpn file as described in the previous section. If you have an separate ca.crt, client.key or client.crt file in your volume mount should be a folder containing both the ca.crt and the .ovpn config. Mount the folder contianing all the required files instead of the openvpn.ovpn file. -v /path/to/your/config/:/etc/openvpn/custom/ Additionally the .ovpn config should include the full path on the docker container to the ca.crt and additional files. ca /etc/openvpn/custom/ca.crt If -e OPENVPN_CONFIG= variable has been omitted from the docker run command the .ovpn config file must be named default.ovpn. If -e OPENVPN_CONFIG= is used with the custom provider the .ovpn config and variable must match as described above.","title":"Supported providers"},{"location":"supported-providers/#adding_new_providers","text":"If your VPN provider is not in the list of supported providers you could always create an issue on GitHub and see if someone could add it for you. But if you're feeling up for doing it yourself, here's a couple of pointers. You clone this repository and create a new folder under \"openvpn\" where you put the .ovpn files your provider gives you. Depending on the structure of these files you need to make some adjustments. For example if they come with a ca.crt file that is referenced in the config you need to update this reference to the path it will have inside the container (which is /etc/openvpn/...). You also have to set where to look for your username/password. There is a script called adjustConfigs.sh that could help you. After putting your .ovpn files in a folder, run that script with your folder name as parameter and it will try to do the changes described above. If you use it or not, reading it might give you some help in what you're looking to change in the .ovpn files. Once you've finished modifying configs, you build the container and run it with OPENVPN_PROVIDER set to the name of the folder of configs you just created (it will be lowercased to match the folder names). And that should be it! So, you've just added your own provider and you're feeling pretty good about it! Why don't you fork this repository, commit and push your changes and submit a pull request? Share your provider with the rest of us! :) Please submit your PR to the dev branch in that case.","title":"Adding new providers"},{"location":"supported-providers/#using_a_custom_provider","text":"If you want to run the image with your own provider without building a new image, that is also possible. For some providers, like AirVPN, the .ovpn files are generated per user and contains credentials. They should not be added to a public image. This is what you do: Add a new volume mount to your docker run command that mounts your config file: -v /path/to/your/config.ovpn:/etc/openvpn/custom/default.ovpn Then you can set OPENVPN_PROVIDER=CUSTOM and the container will use the config you provided. NOTE: Your .ovpn config file probably contains a line that says auth-user-pass . This will prompt OpenVPN to ask for the username and password. As this is running in a scripted environment that is not possible. Change it for auth-user-pass /config/openvpn-credentials.txt which is the file where your OPENVPN_USERNAME and OPENVPN_PASSWORD variables will be written to. If you are using AirVPN or other provider with credentials in the config file, you still need to set OPENVPN_USERNAME and OPENVPN_PASSWORD as this is required by the startup script. They will not be read by the .ovpn file, so you can set them to whatever. Note that you still need to modify your .ovpn file as described in the previous section. If you have an separate ca.crt, client.key or client.crt file in your volume mount should be a folder containing both the ca.crt and the .ovpn config. Mount the folder contianing all the required files instead of the openvpn.ovpn file. -v /path/to/your/config/:/etc/openvpn/custom/ Additionally the .ovpn config should include the full path on the docker container to the ca.crt and additional files. ca /etc/openvpn/custom/ca.crt If -e OPENVPN_CONFIG= variable has been omitted from the docker run command the .ovpn config file must be named default.ovpn. If -e OPENVPN_CONFIG= is used with the custom provider the .ovpn config and variable must match as described above.","title":"Using a custom provider"},{"location":"tips-tricks/","text":"Use Google DNS servers \u00b6 Some have encountered problems with DNS resolving inside the docker container. This causes trouble because OpenVPN will not be able to resolve the host to connect to. If you have this problem use Docker's --dns flag and try using Google's DNS servers by adding --dns 8.8.8.8 --dns 8.8.4.4 as parameters to the usual run command. Restart container if connection is lost \u00b6 If the VPN connection fails or the container for any other reason loses connectivity, you want it to recover from it. One way of doing this is to set environment variable OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60 and use the --restart=always flag when starting the container. This way OpenVPN will exit if ping fails over a period of time which will stop the container and then the Docker deamon will restart it. Let other containers use VPN \u00b6 TODO : Relevant issues... Reach sleep or hybernation on your host if no torrents are active \u00b6 By befault Transmission will always scrape trackers, even if all torrents have completed their activities, or they have been paused manually. This will cause Transmission to be always active, therefore never allow your host server to be inactive and go to sleep/hybernation/whatever. If this is something you want, you can add the following variable when creating the container. It will turn off a hidden setting in Tranmsission which will stop the application to scrape trackers for paused torrents. Transmission will become inactive, and your host will reach the desidered state. -e \"TRANSMISSION_SCRAPE_PAUSED_TORRENTS_ENABLED=false\" Running it on a NAS \u00b6 Several popular NAS platforms supports Docker containers. You should be able to set up and configure this container using their web interfaces. As of version 3.0 of this image creates a TUN interface inside the container by default. This previously had to be mounted from the host which was an issue for some NAS servers. The assumption is that this should now be fixed. If you have issues and the logs seem to blame \"/dev/net/tun\" in some way then you might consider trying to mount a host device and see if that works better. Setting up a TUN device is probably easiest to accomplish by installing an OpenVPN package for the NAS. This should set up the device and you can mount it. Systemd Integration \u00b6 On many modern linux systems, including Ubuntu, systemd can be used to start the transmission-openvpn at boot time, and restart it after any failure. Save the following as /etc/systemd/system/transmission-openvpn.service , and replace the OpenVPN PROVIDER/USERNAME/PASSWORD directives with your settings, and add any other directives that you're using. This service is assuming that there is a bittorrent user set up with a home directory at /home/bittorrent/ . The data directory will be mounted at /home/bittorrent/data/ . This can be changed to whichever user and location you're using. OpenVPN is set to exit if there is a connection failure. OpenVPN exiting triggers the container to also exit, then the Restart=always definition in the transmission-openvpn.service file tells systems to restart things again. [Unit] Description=haugene/transmission-openvpn docker container After=docker.service Requires=docker.service [Service] User=bittorrent TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill transmission-openvpn ExecStartPre=-/usr/bin/docker rm transmission-openvpn ExecStartPre=/usr/bin/docker pull haugene/transmission-openvpn ExecStart=/usr/bin/docker run \\ --name transmission-openvpn \\ --cap-add=NET_ADMIN \\ -v /home/bittorrent/data/:/data \\ -e \"OPENVPN_PROVIDER=TORGUARD\" \\ -e \"OPENVPN_USERNAME=bittorrent@example.com\" \\ -e \"OPENVPN_PASSWORD=hunter2\" \\ -e \"OPENVPN_CONFIG=CA Toronto\" \\ -e \"OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60\" \\ -p 9091:9091 \\ --dns 8.8.8.8 \\ --dns 8.8.4.4 \\ haugene/transmission-openvpn Restart=always RestartSec=5 [Install] WantedBy=multi-user.target Then enable and start the new service with: $ sudo systemctl enable /etc/systemd/system/transmission-openvpn.service $ sudo systemctl restart transmission-openvpn.service If it is stopped or killed in any fashion, systemd will restart the container. If you do want to shut it down, then run the following command and it will stay down until you restart it. $ sudo systemctl stop transmission-openvpn.service # Later ... $ sudo systemctl start transmission-openvpn.service","title":"Tips & Tricks"},{"location":"tips-tricks/#use_google_dns_servers","text":"Some have encountered problems with DNS resolving inside the docker container. This causes trouble because OpenVPN will not be able to resolve the host to connect to. If you have this problem use Docker's --dns flag and try using Google's DNS servers by adding --dns 8.8.8.8 --dns 8.8.4.4 as parameters to the usual run command.","title":"Use Google DNS servers"},{"location":"tips-tricks/#restart_container_if_connection_is_lost","text":"If the VPN connection fails or the container for any other reason loses connectivity, you want it to recover from it. One way of doing this is to set environment variable OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60 and use the --restart=always flag when starting the container. This way OpenVPN will exit if ping fails over a period of time which will stop the container and then the Docker deamon will restart it.","title":"Restart container if connection is lost"},{"location":"tips-tricks/#let_other_containers_use_vpn","text":"TODO : Relevant issues...","title":"Let other containers use VPN"},{"location":"tips-tricks/#reach_sleep_or_hybernation_on_your_host_if_no_torrents_are_active","text":"By befault Transmission will always scrape trackers, even if all torrents have completed their activities, or they have been paused manually. This will cause Transmission to be always active, therefore never allow your host server to be inactive and go to sleep/hybernation/whatever. If this is something you want, you can add the following variable when creating the container. It will turn off a hidden setting in Tranmsission which will stop the application to scrape trackers for paused torrents. Transmission will become inactive, and your host will reach the desidered state. -e \"TRANSMISSION_SCRAPE_PAUSED_TORRENTS_ENABLED=false\"","title":"Reach sleep or hybernation on your host if no torrents are active"},{"location":"tips-tricks/#running_it_on_a_nas","text":"Several popular NAS platforms supports Docker containers. You should be able to set up and configure this container using their web interfaces. As of version 3.0 of this image creates a TUN interface inside the container by default. This previously had to be mounted from the host which was an issue for some NAS servers. The assumption is that this should now be fixed. If you have issues and the logs seem to blame \"/dev/net/tun\" in some way then you might consider trying to mount a host device and see if that works better. Setting up a TUN device is probably easiest to accomplish by installing an OpenVPN package for the NAS. This should set up the device and you can mount it.","title":"Running it on a NAS"},{"location":"tips-tricks/#systemd_integration","text":"On many modern linux systems, including Ubuntu, systemd can be used to start the transmission-openvpn at boot time, and restart it after any failure. Save the following as /etc/systemd/system/transmission-openvpn.service , and replace the OpenVPN PROVIDER/USERNAME/PASSWORD directives with your settings, and add any other directives that you're using. This service is assuming that there is a bittorrent user set up with a home directory at /home/bittorrent/ . The data directory will be mounted at /home/bittorrent/data/ . This can be changed to whichever user and location you're using. OpenVPN is set to exit if there is a connection failure. OpenVPN exiting triggers the container to also exit, then the Restart=always definition in the transmission-openvpn.service file tells systems to restart things again. [Unit] Description=haugene/transmission-openvpn docker container After=docker.service Requires=docker.service [Service] User=bittorrent TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill transmission-openvpn ExecStartPre=-/usr/bin/docker rm transmission-openvpn ExecStartPre=/usr/bin/docker pull haugene/transmission-openvpn ExecStart=/usr/bin/docker run \\ --name transmission-openvpn \\ --cap-add=NET_ADMIN \\ -v /home/bittorrent/data/:/data \\ -e \"OPENVPN_PROVIDER=TORGUARD\" \\ -e \"OPENVPN_USERNAME=bittorrent@example.com\" \\ -e \"OPENVPN_PASSWORD=hunter2\" \\ -e \"OPENVPN_CONFIG=CA Toronto\" \\ -e \"OPENVPN_OPTS=--inactive 3600 --ping 10 --ping-exit 60\" \\ -p 9091:9091 \\ --dns 8.8.8.8 \\ --dns 8.8.4.4 \\ haugene/transmission-openvpn Restart=always RestartSec=5 [Install] WantedBy=multi-user.target Then enable and start the new service with: $ sudo systemctl enable /etc/systemd/system/transmission-openvpn.service $ sudo systemctl restart transmission-openvpn.service If it is stopped or killed in any fashion, systemd will restart the container. If you do want to shut it down, then run the following command and it will stay down until you restart it. $ sudo systemctl stop transmission-openvpn.service # Later ... $ sudo systemctl start transmission-openvpn.service","title":"Systemd Integration"},{"location":"v3/","text":"Version 3.0 released - we have some breaking changes (but not much) \u00b6 Those of you who are following this project knows that we have had some larger changes coming for a while. Hobby projects often get last in line for some love and care, and it took longer than I hoped but here we are. Some highlights on version 3.0: We're dropping the ubuntu based image and making alpine the default (reduce double maintenance) We're making Transmission settings persistent by default, removing the need for all the environment variables (but keeping support for it) We're making it easier to provide your own OpenVPN (.ovpn) config file - adding scripts in the container to modify provider configs as needed to fit the container setup. (still in early stages at this point) We're adding a standardized way to add scripts for doing necessary setup of a provider. This usually means to download a .ovpn config bundle, unpack it and modify it correctly to work in this container. Hopefully these changes will improve the usability of this container. As maintainers we also hope that it will free up time to keep the container up to date and stable instead of managing thousands of .ovpn files coming and going. I'll try to keep a list of breaking changes here, and add to it if we come across more: The CREATE_TUN_DEVICE variable now defaults to true. Mounting /dev/net/tun will lead to an error message in the logs unless you explicitly set it to false. The DOCKER_LOG variable is renamed to LOG_TO_STDOUT If Transmission is running but you can't connect to torrents, try deleting (or rename to .backup) the settings.json file and restart. PS: Now more than ever. We appreciate that you report bugs and issues when you find them. But as there might be more than ususal, please make sure you search and look for a similar one before possibly creating a duplicate. And you can always revert back to the latest tag on the 2.x versions which is 2.14. Instead of running with haugene/transmission-openvpn simply use haugene/transmission-openvpn:2.14 instead. We hope that won't be necessary though :)","title":"Version 3.0 released - we have some breaking changes (but not much)"},{"location":"v3/#version_30_released_-_we_have_some_breaking_changes_but_not_much","text":"Those of you who are following this project knows that we have had some larger changes coming for a while. Hobby projects often get last in line for some love and care, and it took longer than I hoped but here we are. Some highlights on version 3.0: We're dropping the ubuntu based image and making alpine the default (reduce double maintenance) We're making Transmission settings persistent by default, removing the need for all the environment variables (but keeping support for it) We're making it easier to provide your own OpenVPN (.ovpn) config file - adding scripts in the container to modify provider configs as needed to fit the container setup. (still in early stages at this point) We're adding a standardized way to add scripts for doing necessary setup of a provider. This usually means to download a .ovpn config bundle, unpack it and modify it correctly to work in this container. Hopefully these changes will improve the usability of this container. As maintainers we also hope that it will free up time to keep the container up to date and stable instead of managing thousands of .ovpn files coming and going. I'll try to keep a list of breaking changes here, and add to it if we come across more: The CREATE_TUN_DEVICE variable now defaults to true. Mounting /dev/net/tun will lead to an error message in the logs unless you explicitly set it to false. The DOCKER_LOG variable is renamed to LOG_TO_STDOUT If Transmission is running but you can't connect to torrents, try deleting (or rename to .backup) the settings.json file and restart. PS: Now more than ever. We appreciate that you report bugs and issues when you find them. But as there might be more than ususal, please make sure you search and look for a similar one before possibly creating a duplicate. And you can always revert back to the latest tag on the 2.x versions which is 2.14. Instead of running with haugene/transmission-openvpn simply use haugene/transmission-openvpn:2.14 instead. We hope that won't be necessary though :)","title":"Version 3.0 released - we have some breaking changes (but not much)"},{"location":"vpn-networking/","text":"COMING SOON \u00b6 NOTE: This page is just moved from it's previous location. A re-write is coming and I know that there are links to this page that promises more than what's here now. I'm on it (#1558) Access the WebUI \u00b6 But what's going on? My http://my-host:9091 isn't responding? This is because the VPN is active, and since docker is running in a different ip range than your client the response to your request will be treated as \"non-local\" traffic and therefore be routed out through the VPN interface. How to fix this \u00b6 The container supports the LOCAL_NETWORK environment variable. For instance if your local network uses the IP range 192.168.0.0/24 you would pass -e LOCAL_NETWORK=192.168.0.0/24 . Alternatively you can reverse proxy the traffic through another container, as that container would be in the docker range. There is a reverse proxy being built with the container. You can run it using the command below or have a look in the repository proxy folder for inspiration for your own custom proxy. $ docker run -d \\ --link <transmission-container>:transmission \\ -p 8080:8080 \\ haugene/transmission-openvpn-proxy Access the RPC \u00b6 You need to add a / to the end of the URL to be able to connect. Example: http://my-host:9091/transmission/rpc/ Controlling Transmission remotely \u00b6 The container exposes /config as a volume. This is the directory where the supplied transmission and OpenVPN credentials will be stored. If you have transmission authentication enabled and want scripts in another container to access and control the transmission-daemon, this can be a handy way to access the credentials. For example, another container may pause or restrict transmission speeds while the server is streaming video.","title":"VPN and Networking"},{"location":"vpn-networking/#coming_soon","text":"NOTE: This page is just moved from it's previous location. A re-write is coming and I know that there are links to this page that promises more than what's here now. I'm on it (#1558)","title":"COMING SOON"},{"location":"vpn-networking/#access_the_webui","text":"But what's going on? My http://my-host:9091 isn't responding? This is because the VPN is active, and since docker is running in a different ip range than your client the response to your request will be treated as \"non-local\" traffic and therefore be routed out through the VPN interface.","title":"Access the WebUI"},{"location":"vpn-networking/#how_to_fix_this","text":"The container supports the LOCAL_NETWORK environment variable. For instance if your local network uses the IP range 192.168.0.0/24 you would pass -e LOCAL_NETWORK=192.168.0.0/24 . Alternatively you can reverse proxy the traffic through another container, as that container would be in the docker range. There is a reverse proxy being built with the container. You can run it using the command below or have a look in the repository proxy folder for inspiration for your own custom proxy. $ docker run -d \\ --link <transmission-container>:transmission \\ -p 8080:8080 \\ haugene/transmission-openvpn-proxy","title":"How to fix this"},{"location":"vpn-networking/#access_the_rpc","text":"You need to add a / to the end of the URL to be able to connect. Example: http://my-host:9091/transmission/rpc/","title":"Access the RPC"},{"location":"vpn-networking/#controlling_transmission_remotely","text":"The container exposes /config as a volume. This is the directory where the supplied transmission and OpenVPN credentials will be stored. If you have transmission authentication enabled and want scripts in another container to access and control the transmission-daemon, this can be a handy way to access the credentials. For example, another container may pause or restrict transmission speeds while the server is streaming video.","title":"Controlling Transmission remotely"},{"location":"web-proxy/","text":"Web proxy configuration options \u00b6 This container also contains a web-proxy server to allow you to tunnel your web-browser traffic through the same OpenVPN tunnel. This is useful if you are using a private tracker that needs to see you login from the same IP address you are torrenting from. The default listening port is 8888. Note that only ports above 1024 can be specified as all ports below 1024 are privileged and would otherwise require root permissions to run. Remember to add a port binding for your selected (or default) port when starting the container. If you set Username and Password it will enable BasicAuth for the proxy Variable Function Example WEBPROXY_ENABLED Enables the web proxy WEBPROXY_ENABLED=true WEBPROXY_PORT Sets the listening port WEBPROXY_PORT=8888 WEBPROXY_USERNAME Sets the BasicAuth username WEBPROXY_USERNAME=test WEBPROXY_PASSWORD Sets the BasicAuth password WEBPROXY_PASSWORD=password","title":"Web Proxy"},{"location":"web-proxy/#web_proxy_configuration_options","text":"This container also contains a web-proxy server to allow you to tunnel your web-browser traffic through the same OpenVPN tunnel. This is useful if you are using a private tracker that needs to see you login from the same IP address you are torrenting from. The default listening port is 8888. Note that only ports above 1024 can be specified as all ports below 1024 are privileged and would otherwise require root permissions to run. Remember to add a port binding for your selected (or default) port when starting the container. If you set Username and Password it will enable BasicAuth for the proxy Variable Function Example WEBPROXY_ENABLED Enables the web proxy WEBPROXY_ENABLED=true WEBPROXY_PORT Sets the listening port WEBPROXY_PORT=8888 WEBPROXY_USERNAME Sets the BasicAuth username WEBPROXY_USERNAME=test WEBPROXY_PASSWORD Sets the BasicAuth password WEBPROXY_PASSWORD=password","title":"Web proxy configuration options"}]}